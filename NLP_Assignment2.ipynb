{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP/Assignment2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN6f7wYI/+/0UjxjVGFycil",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Manisha1208/NNDL/blob/main/NLP_Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pWdis2S9puKN",
        "outputId": "7b10134e-4648-48fd-f333-121a750ba5eb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'i) Word2vec\\n    ii) USE\\niii)ELMO\\niv) GP2\\nv) Sentence-BERT'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# 1.Convert the above paragraph into vectors using:<br>\n",
        "'''i) Word2vec\n",
        "    ii) USE\n",
        "iii)ELMO\n",
        "iv) GP2\n",
        "v) Sentence-BERT'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_string='''paragraph is a series of sentences that are organized and coherent, and are all  related  to  a  single  topic.  Almost  every  piece  of  writing  you  do  that  is \n",
        "longer  than  a  few  sentences  should  be  organized  into  paragraphs.  This  is because paragraphs show a reader where the subdivisions of an essay begin \n",
        "and end, and thus help the reader see the organization of the essay and grasp its main points.\n",
        "\n",
        "Paragraphs  can  contain  many  different  kinds  of  information.  A  paragraph could  contain  a  series  of  brief  examples  or  a  single  long  illustration  of  a \n",
        "general  point.  It  might  describe  a  place like kolkata,  character,  or  process;  narrate  a series of events; compare or contrast two or more things; classify items into \n",
        "categories;  or  describe  causes  and  effects.  Regardless  of  the  kind  of information they contain, all paragraphs share certain characteristics. One \n",
        "of the most important of these is a topic sentence.'''\n"
      ],
      "metadata": {
        "id": "KjCnyxc_qEiu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "import gensim\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import Word2Vec\n",
        "from gensim import corpora,models,similarities\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5G_caKLzqIt8",
        "outputId": "898b3e2c-1ba7-4307-ea9e-faf436639fb3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#code to convert paragraph to sentences\n",
        "def essay_to_sentences(paragraph):\n",
        "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "    raw_sentences = tokenizer.tokenize(paragraph.strip())\n",
        "    sentences = []\n",
        "    for raw_sentence in raw_sentences:\n",
        "        if len(raw_sentence) > 0:\n",
        "            sentences.append((raw_sentence))\n",
        "    return sentences\n",
        "\n",
        "sentences=essay_to_sentences(my_string)\n",
        "\n",
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWEOB1EVqUkY",
        "outputId": "c932434a-2e9a-41da-c166-de0d9e49f4e3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['paragraph is a series of sentences that are organized and coherent, and are all  related  to  a  single  topic.',\n",
              " 'Almost  every  piece  of  writing  you  do  that  is \\nlonger  than  a  few  sentences  should  be  organized  into  paragraphs.',\n",
              " 'This  is because paragraphs show a reader where the subdivisions of an essay begin \\nand end, and thus help the reader see the organization of the essay and grasp its main points.',\n",
              " 'Paragraphs  can  contain  many  different  kinds  of  information.',\n",
              " 'A  paragraph could  contain  a  series  of  brief  examples  or  a  single  long  illustration  of  a \\ngeneral  point.',\n",
              " 'It  might  describe  a  place like kolkata,  character,  or  process;  narrate  a series of events; compare or contrast two or more things; classify items into \\ncategories;  or  describe  causes  and  effects.',\n",
              " 'Regardless  of  the  kind  of information they contain, all paragraphs share certain characteristics.',\n",
              " 'One \\nof the most important of these is a topic sentence.']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wordvecs=[nltk.word_tokenize(sent) for sent in sentences]\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stops=list(set(stopwords.words(\"english\")))\n",
        "for i in wordvecs:\n",
        "  for j in i:\n",
        "    if j in stops:\n",
        "      i.remove(j)\n",
        "    elif len(j)==1:\n",
        "      i.remove(j)\n",
        "\n",
        "model=gensim.models.Word2Vec(wordvecs,min_count=1,size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qFGDH7kqbcC",
        "outputId": "d09a7faf-63b6-462e-ded7-32af0a14fc1c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #printing vector form of word 'paragraph'\n",
        "model['paragraph']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUkNEO-zqkqg",
        "outputId": "b3e04501-718e-4391-f051-c355b7904453"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.00112849,  0.00809844,  0.00698292, -0.00733131,  0.01485936,\n",
              "        0.00217167, -0.01153485, -0.0147921 , -0.01400384,  0.00171138,\n",
              "       -0.01426927,  0.00750496,  0.00694787, -0.00935511,  0.01003066,\n",
              "        0.0097398 ,  0.00103525, -0.00618113,  0.01153113, -0.01179708,\n",
              "        0.00454916, -0.00134611, -0.0022282 , -0.00120379,  0.00906594,\n",
              "        0.00213953,  0.01167847,  0.01097398, -0.0092687 , -0.01446559,\n",
              "        0.01063533, -0.00648913], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#finding similar words of word 'sentence' in given paragraph\n",
        "model.most_similar('sentence')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvJf9ARQqoaa",
        "outputId": "3d90cd27-3fad-45a9-cee7-20e0eeb0e0b5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('a', 0.4417702555656433),\n",
              " ('subdivisions', 0.38930827379226685),\n",
              " ('coherent', 0.34230226278305054),\n",
              " ('piece', 0.3321799039840698),\n",
              " ('compare', 0.2936052978038788),\n",
              " ('writing', 0.2931091785430908),\n",
              " ('is', 0.26246142387390137),\n",
              " ('more', 0.25402456521987915),\n",
              " ('paragraphs', 0.22909590601921082),\n",
              " ('general', 0.228385329246521)]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub\n",
        "use= hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
        "#converting to vectors\n",
        "embeddings=use(sentences)\n",
        "print(embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UGsAFEoqtGc",
        "outputId": "2cd44deb-72c0-4d0b-e782-ddc95a4fb70a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[ 0.0130397  -0.02448178  0.05377616 ... -0.08659647  0.01437556\n",
            "   0.04036802]\n",
            " [ 0.02902018 -0.00415597  0.07664446 ... -0.06830524 -0.00835191\n",
            "   0.04397805]\n",
            " [ 0.07023076 -0.02952436  0.04878015 ... -0.06741599  0.02838855\n",
            "   0.06126793]\n",
            " ...\n",
            " [ 0.04487116 -0.0484449   0.05696819 ... -0.06665879  0.03268223\n",
            "   0.03346183]\n",
            " [ 0.04141247  0.0258891  -0.00625629 ... -0.0216299   0.00910816\n",
            "   0.03623573]\n",
            " [ 0.00255486 -0.05832284  0.01486255 ... -0.09906971 -0.07840379\n",
            "   0.06731212]], shape=(8, 512), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"shape= \",embeddings[0].shape)\n",
        "#each sentence is converted into vector having 512 values\n",
        "print(\"The sentence: \",sentences[0],\"\\n is converted as : \\n{}\".format(embeddings[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWTWhATOqwvy",
        "outputId": "b6c95110-1649-4a73-9d34-cee0b09001af"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape=  (512,)\n",
            "The sentence:  paragraph is a series of sentences that are organized and coherent, and are all  related  to  a  single  topic. \n",
            " is converted as : \n",
            "[ 0.0130397  -0.02448178  0.05377616  0.09029772 -0.00281371 -0.01281828\n",
            "  0.03285866  0.01635625 -0.06859536  0.06464799 -0.00420633 -0.00475263\n",
            " -0.0616823   0.02333648 -0.0740284  -0.09213066 -0.04730163  0.03868826\n",
            " -0.09117856 -0.0531873   0.0045717   0.06571639 -0.00334152  0.05408446\n",
            " -0.02477303  0.02715091 -0.02349129 -0.05725229 -0.01281677 -0.02824831\n",
            "  0.08104642 -0.01898845  0.00283233 -0.01492393 -0.0671662   0.01855971\n",
            "  0.05093659  0.0259113   0.01541285  0.02317855  0.01918534  0.04582307\n",
            "  0.03908791  0.03328896  0.05890476  0.03568182  0.00344607 -0.05167899\n",
            " -0.03721966  0.02001154  0.00619585  0.05630827  0.00328268 -0.02715196\n",
            " -0.04551012 -0.01019349  0.01659425  0.05470299  0.0246249   0.02216954\n",
            " -0.00490041 -0.03435751  0.05740487  0.06339793  0.05196239  0.00615372\n",
            "  0.04076186 -0.02079652  0.06001855 -0.06635413 -0.00773141  0.0263653\n",
            "  0.02451113  0.06061163 -0.07133552  0.05782957 -0.05754852  0.02895611\n",
            "  0.00151216 -0.01469147 -0.02730503 -0.0703272   0.0395746   0.03485686\n",
            "  0.02526464 -0.00742645  0.02469909 -0.01358036 -0.0916101  -0.04018616\n",
            "  0.02372963 -0.00122874  0.0153879  -0.0321376   0.022159    0.07722456\n",
            " -0.08839002 -0.05269     0.0640604   0.00979441  0.02801462 -0.02621713\n",
            " -0.06385686  0.03499862 -0.00477841  0.00639528  0.05272752  0.03508765\n",
            " -0.05286492 -0.02362025 -0.00963558 -0.0035022   0.01756895  0.03553189\n",
            "  0.00796989  0.0368312  -0.08330484 -0.01369959 -0.0070094  -0.00131602\n",
            "  0.02652308 -0.02945104 -0.03616727 -0.04744886  0.03389478 -0.01550303\n",
            " -0.02648187 -0.01460637  0.05021249 -0.03391631  0.03588076 -0.0231719\n",
            "  0.00961214  0.06751177  0.03312924 -0.08808248 -0.00062285 -0.0764579\n",
            "  0.00174959  0.04190522 -0.06777634  0.03313017  0.0365041   0.0423931\n",
            "  0.04826046  0.01058785 -0.01545047  0.01712772  0.06886434 -0.07422291\n",
            " -0.04627565 -0.07049586  0.03573827  0.0720467   0.02487097 -0.07006394\n",
            " -0.00549838 -0.0134547   0.01732141  0.01763896  0.02833198 -0.03735091\n",
            "  0.06601565  0.07435729  0.01209584 -0.0222442   0.00350412 -0.03343808\n",
            "  0.07319491  0.05448433 -0.00227587  0.08148894  0.00434756 -0.00276034\n",
            " -0.0460109   0.0709954   0.03056395  0.03891557 -0.01200306 -0.06036595\n",
            "  0.0571811  -0.03438857 -0.08607582  0.00783407 -0.03507512  0.02475967\n",
            "  0.00074535 -0.05610893 -0.06465167  0.01065863  0.08055008  0.05711674\n",
            "  0.08142262 -0.00339631  0.02761404 -0.00301872 -0.02929048  0.01929364\n",
            "  0.03403955  0.07205074 -0.00186551  0.03170982 -0.00164836 -0.00137344\n",
            " -0.03302888  0.03679697  0.08747443  0.01423466 -0.02835709 -0.00739498\n",
            " -0.05452731  0.03196729 -0.06697123  0.06497305  0.06452093 -0.00988444\n",
            "  0.00181566  0.05153206  0.06855743  0.03502671 -0.05183265 -0.03011246\n",
            "  0.06948151  0.02025227  0.00705998 -0.03298984  0.03287054  0.00393647\n",
            "  0.04083933 -0.06053992 -0.02506591  0.01384513  0.01519722 -0.04686617\n",
            "  0.08390399  0.0402636   0.08569206  0.00745217  0.02295502 -0.05238653\n",
            " -0.07567251  0.04338444 -0.01321455 -0.0614254   0.03257526 -0.06763536\n",
            " -0.06685274  0.03113586 -0.06481721 -0.035733    0.02733812 -0.08375938\n",
            " -0.01799088 -0.01081532  0.02806066 -0.09438325  0.03154329 -0.03125109\n",
            "  0.01418647 -0.03383907  0.03651096  0.0808764   0.00993911 -0.02245205\n",
            " -0.02083281 -0.00815378  0.02060127 -0.04945804  0.01563843 -0.05327033\n",
            " -0.04862319 -0.06477673  0.02698861  0.03376003  0.04934568  0.02558378\n",
            " -0.07181147  0.02443306  0.06913311  0.09154654  0.06723542  0.0392013\n",
            "  0.06677745 -0.00743725 -0.03869466 -0.01145218  0.01707871 -0.0457652\n",
            "  0.02908334 -0.05101663 -0.03089399  0.01161619 -0.07663381 -0.02881291\n",
            " -0.03894763 -0.04324974 -0.00060032  0.03487562 -0.01907586  0.0376705\n",
            " -0.01158758 -0.04679117  0.05015334 -0.06792013 -0.03622831 -0.04153818\n",
            "  0.0570238  -0.06193868 -0.01113001  0.00617605  0.01305555 -0.03438418\n",
            "  0.03555826 -0.00623084 -0.01022241 -0.04560447 -0.00803091  0.08507649\n",
            "  0.04051197  0.01202952 -0.02420653 -0.06312075  0.05178148 -0.05242546\n",
            " -0.08575398 -0.02616368 -0.08295669  0.07882772  0.03795778 -0.02804446\n",
            "  0.03416594 -0.05801179  0.02940921 -0.02864457 -0.01992084  0.0588765\n",
            " -0.00658678  0.03409409 -0.01784457  0.06945263  0.05324169 -0.01991547\n",
            " -0.02119442 -0.05926772 -0.08899897 -0.01631527  0.07859699  0.06052769\n",
            "  0.01810767  0.00340095 -0.0743404   0.00025741  0.02930632  0.00041092\n",
            "  0.07565875  0.04552222 -0.08557718 -0.02665984  0.01395929 -0.05064283\n",
            "  0.0057656  -0.03037646 -0.06596612  0.00067333  0.00337668  0.05934227\n",
            "  0.0623996  -0.00125538 -0.00902064 -0.04601528 -0.00372166  0.00293051\n",
            "  0.0698199   0.08276602  0.02260012  0.03270639  0.01798311  0.03101933\n",
            "  0.00139869 -0.04988439 -0.03938827 -0.00263859 -0.01390069  0.04127859\n",
            " -0.02217292  0.03091438 -0.07858703 -0.02590636 -0.01385359 -0.03102407\n",
            "  0.00818326 -0.02147414 -0.0433761   0.06724109 -0.00880752 -0.0297972\n",
            "  0.00582617 -0.02213253 -0.01799049  0.03668701  0.0273216   0.03501004\n",
            " -0.04491726  0.05194589  0.01545278  0.0266427  -0.0049839  -0.05031526\n",
            " -0.05752826  0.04363136  0.03929971 -0.02734473  0.05801374 -0.03325567\n",
            " -0.00632119 -0.03009862  0.04856662  0.02217992 -0.00741464 -0.05831642\n",
            "  0.00967851 -0.06455369 -0.07368872  0.01876938  0.03608675  0.04839299\n",
            "  0.03738168  0.08669791 -0.05580652  0.04707275  0.02433234  0.0077433\n",
            "  0.03117906 -0.02164213 -0.02916921 -0.05807215 -0.02118981  0.02184607\n",
            "  0.03851807 -0.08982065 -0.03857014  0.08303563 -0.0696009  -0.01762514\n",
            " -0.01722447 -0.01306598 -0.02174684 -0.00127189  0.07447506 -0.00623246\n",
            "  0.03709365 -0.0050968  -0.07107578 -0.06477595  0.07697952 -0.06396304\n",
            "  0.04565116  0.06125401  0.08525113 -0.06215417 -0.01136644  0.07907545\n",
            " -0.01203242 -0.03026695 -0.02179208 -0.08807361 -0.05830711  0.0134286\n",
            "  0.03525446  0.07980973 -0.03951295 -0.01846914 -0.02684113 -0.00276503\n",
            "  0.0045317   0.01019156  0.0265705  -0.03926333 -0.02621922  0.01123995\n",
            " -0.01552573 -0.01949633  0.00077884 -0.08504637  0.00299779 -0.07721765\n",
            " -0.03524183 -0.05253956  0.04045684  0.02331033  0.03418605  0.00210377\n",
            "  0.04870605  0.01009615  0.03399476  0.0031811  -0.02037844 -0.02973627\n",
            "  0.03287363  0.04293682  0.03946396 -0.04965374  0.0526884   0.08072674\n",
            "  0.05934194 -0.0212247  -0.02222842 -0.04695807  0.06170329 -0.08659647\n",
            "  0.01437556  0.04036802]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert = hub.KerasLayer(\"https://tfhub.dev/google/nnlm-en-dim128/2\")\n",
        "embeddings2=bert(sentences)\n",
        "print(embeddings2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPAJ8DCBq2O5",
        "outputId": "fb0eb69f-e36d-42d4-d573-28e7d51c0306"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[ 0.5779494   0.02082791  0.09225276 ...  0.1423439   0.0289949\n",
            "  -0.15707761]\n",
            " [ 0.5166956   0.00863559  0.13047293 ...  0.06873029  0.04432377\n",
            "   0.02298209]\n",
            " [ 0.7741569   0.20155099 -0.06426485 ... -0.15599789  0.10988081\n",
            "   0.10273374]\n",
            " ...\n",
            " [ 0.49916965 -0.00334622  0.08184471 ...  0.05571959 -0.09832127\n",
            "  -0.04315813]\n",
            " [ 0.2909918   0.06627773  0.08403497 ... -0.10017543 -0.11646989\n",
            "   0.00867226]\n",
            " [ 0.3799187  -0.05745781  0.09742597 ...  0.0461409  -0.07855327\n",
            "   0.03973798]], shape=(8, 128), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"shape=\",embeddings2[0].shape)\n",
        "#each sentence is converted into vector having 128 values\n",
        "print(\"The sentence: \",sentences[0],\"\\n is converted as : \\n{}\".format(embeddings2[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrOIr2Pbq8lz",
        "outputId": "7d813f78-a009-481a-9786-b9677487cb5f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape= (128,)\n",
            "The sentence:  paragraph is a series of sentences that are organized and coherent, and are all  related  to  a  single  topic. \n",
            " is converted as : \n",
            "[ 5.77949405e-01  2.08279062e-02  9.22527611e-02  4.76201549e-02\n",
            " -1.17820695e-04 -7.91035891e-02 -9.09532905e-02 -1.96999479e-02\n",
            " -1.82167172e-01  1.82781354e-01  1.78407803e-02 -1.36595681e-01\n",
            " -1.20970421e-01  3.03527750e-02 -1.55288994e-01 -3.65624130e-02\n",
            " -8.87916684e-02  1.18960990e-02 -2.42571995e-01  1.40147313e-01\n",
            "  6.29373714e-02  1.13711674e-02  2.64876569e-03 -7.81569928e-02\n",
            "  5.36865816e-02 -9.01894644e-02 -6.66456744e-02 -4.82053608e-02\n",
            " -3.14572081e-02  3.31105255e-02  2.45857425e-03 -5.02937846e-02\n",
            " -4.58035916e-02 -2.23965168e-01 -1.51718352e-02 -2.75546517e-02\n",
            "  5.16168959e-02 -1.21403374e-01 -2.82064285e-02  2.14980636e-02\n",
            "  9.37252939e-02 -7.97212273e-02  2.16491118e-01 -4.78220843e-02\n",
            " -5.53186908e-02  1.48325905e-01  1.11704050e-02 -1.39561430e-01\n",
            " -8.59184787e-02 -3.15666497e-02 -7.05443416e-03 -1.22958897e-02\n",
            "  8.78950506e-02  5.11941761e-02  6.96989968e-02 -1.74640752e-02\n",
            "  7.91326687e-02 -9.45335925e-02  6.07643686e-02 -1.13157846e-01\n",
            " -1.14777297e-01  3.14148329e-03 -1.83116067e-02 -9.83897746e-02\n",
            "  3.80383953e-02 -1.43197656e-01 -1.27496868e-01 -6.43403754e-02\n",
            "  8.99146721e-02 -8.34758803e-02  1.38897583e-01 -6.16863295e-02\n",
            " -5.32943830e-02 -5.85685670e-02 -4.70773801e-02  1.35739800e-02\n",
            " -1.84798315e-01 -9.99822170e-02 -1.19276457e-01 -9.13939551e-02\n",
            " -4.48592938e-04 -5.09047136e-02  3.55931111e-02  9.14823189e-02\n",
            " -9.76060778e-02 -2.47048382e-02 -8.19911286e-02  1.33122206e-01\n",
            "  4.12435055e-01  2.50982344e-01 -6.01520836e-02  6.40468746e-02\n",
            " -8.70020315e-02 -2.73808893e-02 -1.16390986e-02  9.75387394e-02\n",
            "  1.02026835e-01 -1.18782625e-01 -1.58835545e-01  9.56854969e-02\n",
            "  1.67436828e-03  9.28485841e-02  1.03814647e-01  1.73505455e-01\n",
            "  1.26846179e-01 -4.27589081e-02  1.42604470e-01 -6.44757226e-02\n",
            " -6.69861436e-02  3.66869792e-02 -2.13260010e-01 -2.61161514e-02\n",
            " -1.13156229e-01 -1.20129973e-01  2.14987561e-01 -1.80747211e-01\n",
            " -7.60586932e-02 -9.45016518e-02 -1.70784760e-02 -1.29153430e-02\n",
            "  8.01100358e-02 -5.87512888e-02 -5.14452979e-02 -1.65604100e-01\n",
            "  4.33626696e-02  1.42343894e-01  2.89948992e-02 -1.57077610e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"tensorflow>=2.0.0\"\n",
        "!pip install --upgrade tensorflow-hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJ9ZbdCbrBHZ",
        "outputId": "00e7d5b0-29f9-462d-b74b-6be63cfabf5f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow>=2.0.0 in /usr/local/lib/python3.7/dist-packages (2.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0) (0.2.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0) (12.0.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0) (1.1.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0) (3.10.0.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0) (1.1.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0) (3.1.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0) (2.7.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0) (1.42.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0) (2.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0) (3.17.3)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0) (1.19.5)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0) (1.6.3)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0) (2.7.0)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0) (0.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0) (1.13.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0) (3.3.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0) (0.12.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0) (0.22.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0) (2.7.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0) (1.15.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow>=2.0.0) (1.5.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0.0) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0.0) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0.0) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0.0) (3.3.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0.0) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0.0) (1.8.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0.0) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0.0) (0.6.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.0.0) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.0.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.0.0) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.0.0) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.0.0) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.0.0) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.0.0) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.0.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.0.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.0.0) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.0.0) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.0.0) (3.1.1)\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.7/dist-packages (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub) (3.17.3)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub) (1.19.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorflow-hub) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_eager_execution()\n",
        "#1024 sized vectors\n",
        "elmo=hub.Module(\"https://tfhub.dev/google/elmo/3\",trainable=True)\n",
        "embeddings=elmo(\n",
        "    sentences,\n",
        "    signature=\"default\",\n",
        "    as_dict=True)[\"elmo\"]\n",
        "init=tf.initialize_all_variables()\n",
        "sess=tf.Session()\n",
        "sess.run(init)\n",
        "print(\"\\n\\n\")\n",
        "print(sess.run(embeddings[0]))\n",
        "print(\"shape=\",embeddings[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVYWiQgDrGLA",
        "outputId": "a5d9ea91-558b-493c-dd0d-da6af6ad667b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/tf_should_use.py:247: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
            "Instructions for updating:\n",
            "Use `tf.global_variables_initializer` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/tf_should_use.py:247: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
            "Instructions for updating:\n",
            "Use `tf.global_variables_initializer` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "[[ 0.12622021  0.43023553  0.4043871  ...  0.22031897  0.26976916\n",
            "   0.46307242]\n",
            " [-0.0062146   0.333897    0.10469673 ... -0.6755089   0.25390008\n",
            "   0.654027  ]\n",
            " [ 0.00899898  0.4038667   0.13564664 ... -0.04539599  0.19543737\n",
            "  -0.34905794]\n",
            " ...\n",
            " [-0.02840841 -0.04353216  0.04130163 ...  0.02583168 -0.01429836\n",
            "  -0.01650422]\n",
            " [-0.02840841 -0.04353216  0.04130163 ...  0.02583168 -0.01429836\n",
            "  -0.01650422]\n",
            " [-0.02840841 -0.04353216  0.04130163 ...  0.02583168 -0.01429836\n",
            "  -0.01650422]]\n",
            "shape= (32, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers"
      ],
      "metadata": {
        "id": "I_DC7xrGrJtg"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gptokenizer=transformers.GPT2Tokenizer.from_pretrained('gpt2-large')\n",
        "model=transformers.GPT2LMHeadModel.from_pretrained('gpt2-large')\n",
        "output=gptokenizer.encode(my_string,add_special_tokens=False,return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "6yeiAe1nrRMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"shape=\",output.shape)\n",
        "output"
      ],
      "metadata": {
        "id": "q9YkZ3CIuTUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Find named entities (NER) for the above paragraph?"
      ],
      "metadata": {
        "id": "aTym9Mw5uZQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "ner=spacy.load('en')\n",
        "result=ner(my_string)\n",
        "\n",
        "for word in result.ents:\n",
        "  print(word.text,word.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1QCZ8ADrXXS",
        "outputId": "3cfe5565-769a-4dbf-93df-2dfb58c61e8d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kolkata PERSON\n",
            "two CARDINAL\n",
            "One CARDINAL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spacy.explain('GPE')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HKQHWH_Ardb5",
        "outputId": "c252c6c1-97bf-450c-e269-679260a97719"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Countries, cities, states'"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#better visualisation of entity recognition\n",
        "displacy.render(result,style=\"ent\",jupyter=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "8jCzr0GTrgmw",
        "outputId": "3ef46c24-f4bb-4bd5-df7e-e01945594925"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">paragraph is a series of sentences that are organized and coherent, and are all  related  to  a  single  topic.  Almost  every  piece  of  writing  you  do  that  is </br>longer  than  a  few  sentences  should  be  organized  into  paragraphs.  This  is because paragraphs show a reader where the subdivisions of an essay begin </br>and end, and thus help the reader see the organization of the essay and grasp its main points.</br></br>Paragraphs  can  contain  many  different  kinds  of  information.  A  paragraph could  contain  a  series  of  brief  examples  or  a  single  long  illustration  of  a </br>general  point.  It  might  describe  a  place like \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    kolkata\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              ",  character,  or  process;  narrate  a series of events; compare or contrast \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    two\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
              "</mark>\n",
              " or more things; classify items into </br>categories;  or  describe  causes  and  effects.  Regardless  of  the  kind  of information they contain, all paragraphs share certain characteristics. \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    One\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
              "</mark>\n",
              " \n",
              "of the most important of these is a topic sentence.</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# finding named entities of another paragraph\n",
        "resultss=ner(\"Cricket was introduced to India by British sailors in the 18th century, and the first cricket club was established in 1792. India's national cricket team did not play its first Test match until 25 June 1932 at Lord's, becoming the sixth team to be granted test cricket status. From 1932 India had to wait until 1952, almost 20 years for its first Test victory. In its first fifty years of international cricket, India didn't gain much success, winning only 35 of the first 196 Test matches it played. The team, however, gained strength in the 1970s with the emergence of players like Gavaskar, Viswanath, Kapil Dev, and the Indian spin quartet.\")\n",
        "for word in resultss.ents:\n",
        "  print(word.text,word.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJ3-QP8NrlY4",
        "outputId": "098b61b2-26ff-4f4f-ed5f-73e18d0411b2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "India GPE\n",
            "British NORP\n",
            "the 18th century DATE\n",
            "first ORDINAL\n",
            "1792 DATE\n",
            "India GPE\n",
            "first ORDINAL\n",
            "25 June 1932 DATE\n",
            "sixth ORDINAL\n",
            "1932 DATE\n",
            "India GPE\n",
            "1952 DATE\n",
            "almost 20 years DATE\n",
            "first ORDINAL\n",
            "its first fifty years DATE\n",
            "India GPE\n",
            "only 35 CARDINAL\n",
            "first ORDINAL\n",
            "196 CARDINAL\n",
            "the 1970s DATE\n",
            "Gavaskar PERSON\n",
            "Viswanath ORG\n",
            "Kapil Dev PERSON\n",
            "Indian NORP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#better visualisation of entity recognition\n",
        "displacy.render(resultss,style=\"ent\",jupyter=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "_RGwpz0_rs7J",
        "outputId": "933ff1cd-079e-4c3a-9dbd-03239cef6333"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Cricket was introduced to \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    India\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " by \n",
              "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    British\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
              "</mark>\n",
              " sailors in \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    the 18th century\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ", and the \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    first\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
              "</mark>\n",
              " cricket club was established in \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    1792\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ". \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    India\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              "'s national cricket team did not play its \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    first\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
              "</mark>\n",
              " Test match until \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    25 June 1932\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              " at Lord's, becoming the \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    sixth\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
              "</mark>\n",
              " team to be granted test cricket status. From \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    1932\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              " \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    India\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " had to wait until \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    1952\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ", \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    almost 20 years\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              " for its \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    first\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
              "</mark>\n",
              " Test victory. In \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    its first fifty years\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              " of international cricket, \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    India\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " didn't gain much success, winning \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    only 35\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
              "</mark>\n",
              " of the \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    first\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
              "</mark>\n",
              " \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    196\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
              "</mark>\n",
              " Test matches it played. The team, however, gained strength in \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    the 1970s\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              " with the emergence of players like \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Gavaskar\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              ", \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Viswanath\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ", \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Kapil Dev\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              ", and the \n",
              "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Indian\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
              "</mark>\n",
              " spin quartet.</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Find similar sentences(repeated sentences) from the above paragraph?\n",
        "pip install -U sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')"
      ],
      "metadata": {
        "id": "vYD37KDMrySr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "se_embeddings = sbert_model.encode(sentences)\n",
        "q1_vec= sbert_model.encode(sentences[0])\n",
        "\n",
        "#cosine similarity function\n",
        "#identifies similarity between 2 sentences\n",
        "def cosine(u, v):\n",
        "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
        "\n",
        "for sent in sentences:\n",
        "  sim = cosine(q1_vec, sbert_model.encode([sent])[0])\n",
        "  #if similarity ==1 => repeated sentence\n",
        "  #if similarity > 0.6 => similar sentence\n",
        "  if sim>0.6:\n",
        "    print(\"Sentence1 =\",sentences[0],\"\\n \\nSentence2=\", sent, \"\\n\\nsimilarity = \", sim,end=\"\\n ----------------------------- \\n\")"
      ],
      "metadata": {
        "id": "EltTnSZXr2cV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Explain POS tagging with HMM?"
      ],
      "metadata": {
        "id": "2vbzNFsUr3o8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parts of Speech Tagging (POS): It is a process of converting a sentence to forms  list of words, list of tuples (where each tuple is having a form (word, tag)). The tag in case of is a part-of-speech tag, and signifies whether the word is a noun, adjective, verb, and so on. reading a sentence and being able to identify what words act as nouns, pronouns, verbs, adverbs, and so on. All these are referred to as the part of speech tags.\n",
        " According to Wikipedia, part-of-speech tagging (POS tagging or PoS tagging or POST), also called grammatical tagging or word-category disambiguation, is the process of marking up a word in a text as corresponding to a particular part of speech, based on both its definition and its context i.e. its relationship with adjacent and related words in a phrase, sentence, or paragraph.\n",
        "\n",
        "image.png\n",
        "\n",
        "Default tagging is a basic step for the part-of-speech tagging. It is performed using the DefaultTagger class. The DefaultTagger class takes tag as a single argument. NN is the tag for a singular noun. DefaultTagger is most useful when it gets to work with most common part-of-speech tag. thats why a noun tag is recommended.\n",
        "\n",
        "image.png\n",
        "\n",
        "POS tagging with Hidden Markov Model\n",
        "HMM (Hidden Markov Model) is a Stochastic technique for POS tagging. Hidden Markov models are known for their applications to reinforcement learning and temporal pattern recognition such as speech, handwriting, gesture recognition, musical score following, partial discharges, and bioinformatics.\n",
        "\n",
        "Let us consider an example proposed by Dr.Luis Serrano and find out how HMM selects an appropriate tag sequence for a sentence.\n",
        "\n",
        "\n",
        "In this example, we consider only 3 POS tags that are noun, model and verb. Let the sentence  Ted will spot Will  be tagged as noun, model, verb and a noun and to calculate the probability associated with this particular sequence of tags we require their Transition probability and Emission probability.\n",
        "\n",
        "The transition probability is the likelihood of a particular sequence for example, how likely is that a noun is followed by a model and a model by a verb and a verb by a noun. This probability is known as Transition probability. It should be high for a particular sequence to be correct.\n",
        "\n",
        "Now, what is the probability that the word Ted is a noun, will is a model, spot is a verb and Will is a noun. These sets of probabilities are Emission probabilities and should be high for our tagging to be likely.\n",
        "\n",
        "Let us calculate the above two probabilities for the set of sentences below\n",
        "\n",
        "Mary Jane can see Will\n",
        "Spot will see Mary\n",
        "Will Jane spot Mary?\n",
        "Mary will pat Spot\n",
        "Note that Mary Jane, Spot, and Will are all names.\n",
        "\n",
        "\n",
        "In the above sentences, the word Mary appears four times as a noun. and see appears two times as a verb. we need to calculate the probabilitiy of a word appearing as noun, verb or model. to do this, we need to calculate the emission probabilities, which represented using below table.\n",
        "\n",
        "Screenshot 2021-11-23 144217.jpg\n",
        "\n",
        "Now divide each column by the total number of their appearances .for example, noun appears nine times in the above sentences, so divide each term by 9 in the noun column. and repeat the same for all remaining processes. We get the following table after this operation.\n",
        "\n",
        "Screenshot 2021-11-23 141155.jpg\n",
        "\n",
        "From the above table, we can conclude that\n",
        "\n",
        "The probability that Mary is Noun = 4/9\n",
        "\n",
        "The probability that Mary is Model = 0\n",
        "\n",
        "The probability that Mary is Verb = 0\n",
        "\n",
        "The probability that Will is Noun = 1/9\n",
        "\n",
        "The probability that Will is Model = 3/4\n",
        "\n",
        "In a similar manner, we can analyze rest of the probabilities. These are the emission probabilities.\n",
        "\n",
        "Next, we have to calculate the transition probabilities, so define two more tags < S > and < E >. < S > is placed at the beginning of each sentence and < E > at the end as shown in the figure below.\n",
        "\n",
        "since for first and last word there is no previous and next words, so we are adding extra dummy words. i.e < E > and < S >\n",
        "\n",
        "\n",
        "now we need to create a table and fill it with the co-occurrence counts of the tags.\n",
        "\n",
        "Screenshot 2021-11-23 142823.jpg\n",
        "\n",
        "In the above figure, we can see that the < S > tag is followed by the N tag three times, thus the first entry is 3.The model tag follows the < S > just once, thus the second entry is 1. In a similar manner, the rest of the table is filled.\n",
        "\n",
        "Next, we divide each term in a row of the table by the total number of co-occurrences of the tag in consideration, for example, The Model tag is followed by any other tags four times (in total) as shown below, thus we divide each element in the third row by four.\n",
        "\n",
        "\n",
        "the table is refined as below:\n",
        "\n",
        "Screenshot 2021-11-23 143123.jpg\n",
        "\n",
        "These are the respective transition probabilities for the above four sentences.\n",
        "\n",
        "how does the HMM determine the appropriate sequence of tags for a particular sentence from the above tables? Let us find it out.\n",
        "\n",
        "Take a new sentence and tag them with wrong tags. Let the sentence,  Will can spot Mary be tagged as-\n",
        "\n",
        "Will as a model Can as a verb Spot as a noun Mary as a noun Now calculate the probability of this sequence being correct in the following manner.\n",
        "\n",
        "image.png\n",
        "\n",
        "The probability of the tag Model (M) comes after the tag is  as seen in the table. Also, the probability that the word Will is a Model is 3/4. In the same manner, we calculate each and every probability in the graph. Now the product of these probabilities is the likelihood that this sequence is right. Since the tags are not correct, the product is zero.\n",
        "\n",
        "1/43/43/4012/91/94/94/9=0\n",
        "\n",
        "When these words are correctly tagged, we get a probability greater than zero as shown below\n",
        "\n",
        "image.png\n",
        "\n",
        "Calculating the product of these terms we get,\n",
        "\n",
        "3/41/93/91/43/41/414/94/9=0.00025720164\n",
        "\n",
        "For our example, keeping into consideration just three POS tags we have mentioned, 81 different combinations of tags can be formed. In this case, calculating the probabilities of all 81 combinations seems achievable. But when the task is to tag a larger sentence and all the POS tags in the Penn Treebank project are taken into consideration, the number of possible combinations grows exponentially and this task seems impossible to achieve. Now let us visualize these 81 combinations as paths and using the transition and emission probability mark each vertex and edge as shown below.\n",
        "\n",
        "image.png\n",
        "\n",
        "The next step is to delete all the vertices and edges with probability zero, also the vertices which do not lead to the endpoint are removed. Also, we will mention-\n",
        "\n",
        "image.png\n",
        "\n",
        "Now there are only two paths that lead to the end, let us calculate the probability associated with each path.\n",
        "\n",
        "sNMNN E =3/41/93/91/41/42/91/94/94/9=0.00000846754\n",
        "\n",
        "sNMNVE=3/41/93/91/43/41/414/94/9=0.00025720164\n",
        "\n",
        "Clearly, the probability of the second sequence is much higher and hence the HMM is going to tag each word in the sentence according to this sequence."
      ],
      "metadata": {
        "id": "49gLtumosH8p"
      }
    }
  ]
}